{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 导入模块",
   "id": "267ed3bc7805392d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T06:01:24.386905Z",
     "start_time": "2024-06-13T06:01:21.847237Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "os.chdir(\"D:/WorkPath/PycharmProjects/MutTm-pred\")\n",
    "from Dataset.Process4Dataset.DatasetCeator4PonDT import Dataset4MutTm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 25",
   "id": "fd1c6d91e8b7ecea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T01:18:43.225727Z",
     "start_time": "2024-06-06T01:14:52.123852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集生成\n",
    "dataset = Dataset4MutTm(package_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\Process4Dataset\",\n",
    "                          train_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\PonDB\\pH-Tm\\PonDB.csv\",\n",
    "                          test_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\ProThermDB\\pH-Tm\\excllent_ProThermDB_Testing.csv\",\n",
    "                          training_version=\"PonDB_withpHTm\",\n",
    "                          testing_version=\"ProThermDBTest_withpHTm\",\n",
    "                          selected_columns=[\"UniProt_ID\", \"Mutation\", \"pH\", \"Tm\", \"ΔTm\"],\n",
    "                          features=[\"neighbor\", \"aaindex\", \"group\", \"param\", \"rpm\",\n",
    "                                    \"sift4g\", \"hydrop\", \"context_embedding\"],\n",
    "                          context_length=25,\n",
    "                          embedding_model_path=\"DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D\",\n",
    "                          embedding_model_name=\"ESM-2-150M\",\n",
    "                          embedding_method=\"add\")\n",
    "train_feature = np.array(dataset.train_feature_set)\n",
    "train_label = np.array(dataset.train_label_set).ravel()\n",
    "\n",
    "# 特征筛选\n",
    "best_rfe = RFE(XGBRegressor(),\n",
    "              n_features_to_select=200,\n",
    "              step=10,\n",
    "              verbose=False)\n",
    "best_rfe.fit(train_feature, train_label)\n",
    "\n",
    "# 模型训练\n",
    "best_model = XGBRegressor(max_depth=23,\n",
    "                          min_child_weight=7,\n",
    "                          gamma=0.67,\n",
    "                          subsample=0.83,\n",
    "                          colsample_bytree=0.93,\n",
    "                          alpha=0.0175)\n",
    "best_model.fit(best_rfe.transform(train_feature), train_label)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(best_rfe, \"ModelSave/rfe_25.pkl\")\n",
    "joblib.dump(best_model, \"ModelSave/model_25.pkl\")"
   ],
   "id": "23554ca712827c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备：  NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "===正在从训练集版本为PonDB_withpHTm、测试集版本为ProThermDBTest_withpHTm的原始数据集中进行数据清洗和生物特征提取工作===\n",
      "1.预处理训练集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-前一数据集采用了后一数据集中的0条数据，现已删除\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了49个UniProtID无法获取序列的数据\n",
      "-删除了15条非法长度的数据，当前蛋白质长度被限制在(25, 5000)\n",
      "-删除条875个突变位点对应错误的数据\n",
      "2.预处理测试集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了0个UniProtID无法获取序列的数据\n",
      "-删除了0条非法长度的数据，当前蛋白质长度被限制在(25, 5000)\n",
      "-删除条16个突变位点对应错误的数据\n",
      "3.为训练集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5546298027038574\n",
      "-释放模型后显存用量: 0.008134841918945312\n",
      "4.为测试集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.008134841918945312\n",
      "6.从全数据集中提取生物特征集、标签集和基本信息集...\n",
      "7.数据清洗和生物特征提取工作完成==>当前显存用量:0.0079345703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ModelSave/model_25.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 50",
   "id": "5186bedbf1e1886e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T06:05:18.817550Z",
     "start_time": "2024-06-13T06:01:28.320103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集生成\n",
    "dataset = Dataset4MutTm(package_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\Process4Dataset\",\n",
    "                          train_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\PonDB\\pH-Tm\\PonDB.csv\",\n",
    "                          test_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\ProThermDB\\pH-Tm\\excllent_ProThermDB_Testing.csv\",\n",
    "                          training_version=\"PonDB_withpHTm\",\n",
    "                          testing_version=\"ProThermDBTest_withpHTm\",\n",
    "                          selected_columns=[\"UniProt_ID\", \"Mutation\", \"pH\", \"Tm\", \"ΔTm\"],\n",
    "                          features=[\"neighbor\", \"aaindex\", \"group\", \"param\", \"rpm\",\n",
    "                                    \"sift4g\", \"hydrop\", \"context_embedding\"],\n",
    "                          context_length=50,\n",
    "                          embedding_model_path=\"DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D\",\n",
    "                          embedding_model_name=\"ESM-2-150M\",\n",
    "                          embedding_method=\"add\")\n",
    "train_feature = np.array(dataset.train_feature_set)\n",
    "train_label = np.array(dataset.train_label_set).ravel()\n",
    "\n",
    "# 特征筛选\n",
    "best_rfe = RFE(XGBRegressor(),\n",
    "              n_features_to_select=200,\n",
    "              step=10,\n",
    "              verbose=False)\n",
    "best_rfe.fit(train_feature, train_label)\n",
    "\n",
    "# 模型训练\n",
    "best_model = XGBRegressor(max_depth=23,\n",
    "                          min_child_weight=7,\n",
    "                          gamma=0.67,\n",
    "                          subsample=0.83,\n",
    "                          colsample_bytree=0.93,\n",
    "                          alpha=0.0175)\n",
    "best_model.fit(best_rfe.transform(train_feature), train_label)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(best_rfe, \"ModelSave/rfe_50.pkl\")\n",
    "joblib.dump(best_model, \"ModelSave/model_50.pkl\")"
   ],
   "id": "d5b99207261ea6da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备：  NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "===正在从训练集版本为PonDB_withpHTm、测试集版本为ProThermDBTest_withpHTm的原始数据集中进行数据清洗和生物特征提取工作===\n",
      "1.预处理训练集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-前一数据集采用了后一数据集中的0条数据，现已删除\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了49个UniProtID无法获取序列的数据\n",
      "-删除了15条非法长度的数据，当前蛋白质长度被限制在(50, 5000)\n",
      "-删除条875个突变位点对应错误的数据\n",
      "2.预处理测试集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了0个UniProtID无法获取序列的数据\n",
      "-删除了0条非法长度的数据，当前蛋白质长度被限制在(50, 5000)\n",
      "-删除条16个突变位点对应错误的数据\n",
      "3.为训练集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5546298027038574\n",
      "-释放模型后显存用量: 0.008306503295898438\n",
      "4.为测试集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.008306503295898438\n",
      "6.从全数据集中提取生物特征集、标签集和基本信息集...\n",
      "7.数据清洗和生物特征提取工作完成==>当前显存用量:0.0079345703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ModelSave/model_50.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 100",
   "id": "4faf5bab0fb8da54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T01:26:11.105385Z",
     "start_time": "2024-06-06T01:22:33.571019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集生成\n",
    "dataset = Dataset4MutTm(package_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\Process4Dataset\",\n",
    "                          train_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\PonDB\\pH-Tm\\PonDB.csv\",\n",
    "                          test_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\ProThermDB\\pH-Tm\\excllent_ProThermDB_Testing.csv\",\n",
    "                          training_version=\"PonDB_withpHTm\",\n",
    "                          testing_version=\"ProThermDBTest_withpHTm\",\n",
    "                          selected_columns=[\"UniProt_ID\", \"Mutation\", \"pH\", \"Tm\", \"ΔTm\"],\n",
    "                          features=[\"neighbor\", \"aaindex\", \"group\", \"param\", \"rpm\",\n",
    "                                    \"sift4g\", \"hydrop\", \"context_embedding\"],\n",
    "                          context_length=100,\n",
    "                          embedding_model_path=\"DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D\",\n",
    "                          embedding_model_name=\"ESM-2-150M\",\n",
    "                          embedding_method=\"add\")\n",
    "train_feature = np.array(dataset.train_feature_set)\n",
    "train_label = np.array(dataset.train_label_set).ravel()\n",
    "\n",
    "# 特征筛选\n",
    "best_rfe = RFE(XGBRegressor(),\n",
    "              n_features_to_select=200,\n",
    "              step=10,\n",
    "              verbose=False)\n",
    "best_rfe.fit(train_feature, train_label)\n",
    "\n",
    "# 模型训练\n",
    "best_model = XGBRegressor(max_depth=23,\n",
    "                          min_child_weight=7,\n",
    "                          gamma=0.67,\n",
    "                          subsample=0.83,\n",
    "                          colsample_bytree=0.93,\n",
    "                          alpha=0.0175)\n",
    "best_model.fit(best_rfe.transform(train_feature), train_label)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(best_rfe, \"ModelSave/rfe_100.pkl\")\n",
    "joblib.dump(best_model, \"ModelSave/model_100.pkl\")"
   ],
   "id": "9a4d8c849264b525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备：  NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "===正在从训练集版本为PonDB_withpHTm、测试集版本为ProThermDBTest_withpHTm的原始数据集中进行数据清洗和生物特征提取工作===\n",
      "1.预处理训练集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-前一数据集采用了后一数据集中的0条数据，现已删除\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了49个UniProtID无法获取序列的数据\n",
      "-删除了411条非法长度的数据，当前蛋白质长度被限制在(100, 5000)\n",
      "-删除条841个突变位点对应错误的数据\n",
      "2.预处理测试集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了0个UniProtID无法获取序列的数据\n",
      "-删除了38条非法长度的数据，当前蛋白质长度被限制在(100, 5000)\n",
      "-删除条15个突变位点对应错误的数据\n",
      "3.为训练集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n",
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.008678436279296875\n",
      "4.为测试集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.008678436279296875\n",
      "6.从全数据集中提取生物特征集、标签集和基本信息集...\n",
      "7.数据清洗和生物特征提取工作完成==>当前显存用量:0.0079345703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ModelSave/model_100.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 200",
   "id": "2aeb482e17dae53f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T01:29:30.218925Z",
     "start_time": "2024-06-06T01:26:11.106382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集生成\n",
    "dataset = Dataset4MutTm(package_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\Process4Dataset\",\n",
    "                          train_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\PonDB\\pH-Tm\\PonDB.csv\",\n",
    "                          test_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\ProThermDB\\pH-Tm\\excllent_ProThermDB_Testing.csv\",\n",
    "                          training_version=\"PonDB_withpHTm\",\n",
    "                          testing_version=\"ProThermDBTest_withpHTm\",\n",
    "                          selected_columns=[\"UniProt_ID\", \"Mutation\", \"pH\", \"Tm\", \"ΔTm\"],\n",
    "                          features=[\"neighbor\", \"aaindex\", \"group\", \"param\", \"rpm\",\n",
    "                                    \"sift4g\", \"hydrop\", \"context_embedding\"],\n",
    "                          context_length=200,\n",
    "                          embedding_model_path=\"DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D\",\n",
    "                          embedding_model_name=\"ESM-2-150M\",\n",
    "                          embedding_method=\"add\")\n",
    "train_feature = np.array(dataset.train_feature_set)\n",
    "train_label = np.array(dataset.train_label_set).ravel()\n",
    "\n",
    "# 特征筛选\n",
    "best_rfe = RFE(XGBRegressor(),\n",
    "              n_features_to_select=200,\n",
    "              step=10,\n",
    "              verbose=False)\n",
    "best_rfe.fit(train_feature, train_label)\n",
    "\n",
    "# 模型训练\n",
    "best_model = XGBRegressor(max_depth=23,\n",
    "                          min_child_weight=7,\n",
    "                          gamma=0.67,\n",
    "                          subsample=0.83,\n",
    "                          colsample_bytree=0.93,\n",
    "                          alpha=0.0175)\n",
    "best_model.fit(best_rfe.transform(train_feature), train_label)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(best_rfe, \"ModelSave/rfe_200.pkl\")\n",
    "joblib.dump(best_model, \"ModelSave/model_200.pkl\")"
   ],
   "id": "8a2066a94f071a23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备：  NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "===正在从训练集版本为PonDB_withpHTm、测试集版本为ProThermDBTest_withpHTm的原始数据集中进行数据清洗和生物特征提取工作===\n",
      "1.预处理训练集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-前一数据集采用了后一数据集中的0条数据，现已删除\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了49个UniProtID无法获取序列的数据\n",
      "-删除了2989条非法长度的数据，当前蛋白质长度被限制在(200, 5000)\n",
      "-删除条379个突变位点对应错误的数据\n",
      "2.预处理测试集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了0个UniProtID无法获取序列的数据\n",
      "-删除了137条非法长度的数据，当前蛋白质长度被限制在(200, 5000)\n",
      "-删除条10个突变位点对应错误的数据\n",
      "3.为训练集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n",
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.009393692016601562\n",
      "4.为测试集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.009393692016601562\n",
      "6.从全数据集中提取生物特征集、标签集和基本信息集...\n",
      "7.数据清洗和生物特征提取工作完成==>当前显存用量:0.0079345703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ModelSave/model_200.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 500",
   "id": "ec00b3d6dda7f41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T01:31:44.830439Z",
     "start_time": "2024-06-06T01:29:30.219925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集生成\n",
    "dataset = Dataset4MutTm(package_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\Process4Dataset\",\n",
    "                          train_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\PonDB\\pH-Tm\\PonDB.csv\",\n",
    "                          test_dataset_path=r\"D:\\WorkPath\\PycharmProjects\\MutTm-pred\\Dataset\\BasicData\\ProThermDB\\pH-Tm\\excllent_ProThermDB_Testing.csv\",\n",
    "                          training_version=\"PonDB_withpHTm\",\n",
    "                          testing_version=\"ProThermDBTest_withpHTm\",\n",
    "                          selected_columns=[\"UniProt_ID\", \"Mutation\", \"pH\", \"Tm\", \"ΔTm\"],\n",
    "                          features=[\"neighbor\", \"aaindex\", \"group\", \"param\", \"rpm\",\n",
    "                                    \"sift4g\", \"hydrop\", \"context_embedding\"],\n",
    "                          context_length=500,\n",
    "                          embedding_model_path=\"DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D\",\n",
    "                          embedding_model_name=\"ESM-2-150M\",\n",
    "                          embedding_method=\"add\")\n",
    "train_feature = np.array(dataset.train_feature_set)\n",
    "train_label = np.array(dataset.train_label_set).ravel()\n",
    "\n",
    "# 特征筛选\n",
    "best_rfe = RFE(XGBRegressor(),\n",
    "              n_features_to_select=200,\n",
    "              step=10,\n",
    "              verbose=False)\n",
    "best_rfe.fit(train_feature, train_label)\n",
    "\n",
    "# 模型训练\n",
    "best_model = XGBRegressor(max_depth=23,\n",
    "                          min_child_weight=7,\n",
    "                          gamma=0.67,\n",
    "                          subsample=0.83,\n",
    "                          colsample_bytree=0.93,\n",
    "                          alpha=0.0175)\n",
    "best_model.fit(best_rfe.transform(train_feature), train_label)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(best_rfe, \"ModelSave/rfe_500.pkl\")\n",
    "joblib.dump(best_model, \"ModelSave/model_500.pkl\")"
   ],
   "id": "75911bb764f920db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备：  NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "===正在从训练集版本为PonDB_withpHTm、测试集版本为ProThermDBTest_withpHTm的原始数据集中进行数据清洗和生物特征提取工作===\n",
      "1.预处理训练集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-前一数据集采用了后一数据集中的0条数据，现已删除\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了49个UniProtID无法获取序列的数据\n",
      "-删除了5982条非法长度的数据，当前蛋白质长度被限制在(500, 5000)\n",
      "-删除条141个突变位点对应错误的数据\n",
      "2.预处理测试集数据...\n",
      "-删除数据缺失行及非法行共计0行\n",
      "-丢弃pH/Tm的缺失值\n",
      "-获取[序列]信息.....该数据集已经经过处理，直接使用缓存文件\n",
      "-删除了0个UniProtID无法获取序列的数据\n",
      "-删除了317条非法长度的数据，当前蛋白质长度被限制在(500, 5000)\n",
      "-删除条8个突变位点对应错误的数据\n",
      "3.为训练集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.011539459228515625\n",
      "4.为测试集数据提取生物特征...\n",
      "-获取[neighbor特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[aaindex特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[group特征].....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[sift4g]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[param]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[pssm]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[hydrop]特征.....该数据集已经经过处理，直接使用缓存文件\n",
      "-获取[context_embedding]特征,该特征没有缓存文件，直接生成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at DeepLearning/EmbeddingModels/ESM-2/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入模型后显存使用量:  0.5622639656066895\n",
      "-释放模型后显存用量: 0.011539459228515625\n",
      "6.从全数据集中提取生物特征集、标签集和基本信息集...\n",
      "7.数据清洗和生物特征提取工作完成==>当前显存用量:0.0079345703125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ModelSave/model_500.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
